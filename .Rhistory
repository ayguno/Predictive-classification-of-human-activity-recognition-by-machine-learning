modFit <- train(wage ~ ., method = "gbm", data = training, verbose = FALSE)
print(modFit)
plot(modFit)
plot(modFit$finalModel)
modFit$finalModel
modFit$finalModel$trees
plot(modFit$finalModel$trees)
plot(modFit$finalModel$c.splits)
library(ISLR); data(Wage); library(ggplot2);library(caret);
Wage <- subset(Wage, select = -c(logwage))
inTrain <- createDataPartition(y = Wage$wage, p = 0.7, list = FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
# Again we use the train( ) function, fitting the data only using the training set:
modFit <- train(wage ~ ., method = "gbm", data = training, verbose = FALSE)
print(modFit)
qplot(predict(modFit,testing), wage, data = testing)+theme_bw()
data(iris);library(ggplot2);names(iris);table(iris$Species)
library(caret)
inTrain <- createDataPartition(y = iris$Species, p = 0.7, list = FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training);dim(testing)
modlda <- train(Species ~ . , data = training, method = "lda")
modnb <- train(Species ~ . , data = training, method = "nb")
library(caret)
inTrain <- createDataPartition(y = iris$Species, p = 0.7, list = FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training);dim(testing)
# Chunk 1
library(kernlab)
data("spam")
# Chunk 2
library(kernlab)
data("spam")
head(spam)
# Chunk 3
plot(density(spam$your[spam$type == "nonspam"]),col="navy",main = "", xlab = "Frequency of 'your'",lwd = 2)
lines(density(spam$your[spam$type == "spam"]),col="red", lwd =2)
legend("topright", legend = c("non-spam", "spam"), col = c("navy", "red"), lwd = 2)
# Chunk 4
plot(density(spam$your[spam$type == "nonspam"]),col="navy",main = "", xlab = "Frequency of 'your'",lwd = 2)
lines(density(spam$your[spam$type == "spam"]),col="red", lwd =2)
legend("topright", legend = c("non-spam", "spam"), col = c("navy", "red"), lwd = 2)
abline(v = 0.5, col = "black", lwd = 2)
# Chunk 5
prediction <- ifelse(spam$your > 0.5, "spam", "nonspam")
table(prediction,spam$type)/length(spam$type)
# Chunk 6
library(kernlab);data("spam");set.seed(333)
smallSpam <- spam[sample(dim(spam)[1], size = 10),] #sample size of 10 from spam
spamLabel <- (smallSpam$type == "spam") *1 +1 # a new variable, 2 when spam, 1 when not
plot(smallSpam$capitalAve, col=spamLabel, pch=19) # We plot the feature: Average number of capital letters seen in an e-mail
# Chunk 7
rule1 <- function(x){
prediction <- rep(NA, length(x))
prediction[x > 2.7] <- "spam"
prediction[x < 2.7] <- "nonspam"
prediction[x <= 2.45 & x >= 2.4] <- "spam"
prediction[x >= 2.45 & x <= 2.7] <- "nonspam"
return(prediction)
}
table(rule1(smallSpam$capitalAve),smallSpam$type)
# Chunk 8
rule2 <- function(x){
prediction <- rep(NA, length(x))
prediction[x > 2.8] <- "spam"
prediction[x <= 2.8] <- "nonspam"
return(prediction)
}
table(rule2(smallSpam$capitalAve),smallSpam$type)
# Chunk 9
table(rule1(spam$capitalAve),spam$type)
table(rule2(spam$capitalAve),spam$type)
# Chunk 10
sum(rule1(spam$capitalAve) == spam$type) # number of times our prediction is equal to real value
sum(rule2(spam$capitalAve)== spam$type) # number of times our prediction is equal to real value
# Chunk 11
sum(rule1(spam$capitalAve) == spam$type) /length(spam$type)
sum(rule2(spam$capitalAve)== spam$type) /length(spam$type)
# Chunk 12
library(caret);library(kernlab);data(spam)
inTrain <- createDataPartition(y = spam$type, p = 0.75, list = FALSE) # returns the selected index values for the training data set
# y : vector of outcomes
# p : the percentage of data that goes to training
# list = FALSE we don't want the results in the form of list
training <- spam[inTrain,] # use indexes to choose the training set
testing <- spam[-inTrain,] # subtract indexes to define the testing set
dim(training)
# Chunk 13
set.seed(32343)
modelFit <- train(type~.,data = training, method = "glm")
modlda <- train(Species ~ . , data = training, method = "lda")
modnb <- train(Species ~ . , data = training, method = "nb")
plda <- predict(modlda, testing)
pnb <- predict(modnb, testing)
table(plda, pnb)
modlda <- train(Species ~ . , data = training, method = "lda")
modnb <- train(Species ~ . , data = training, method = "nb")
plda <- predict(modlda, testing)
pnb <- predict(modnb, testing)
table(plda, pnb)
data(iris);library(ggplot2);names(iris);table(iris$Species)
library(caret)
inTrain <- createDataPartition(y = iris$Species, p = 0.7, list = FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training);dim(testing)
modlda <- train(Species ~ . , data = training, method = "lda")
modnb <- train(Species ~ . , data = training, method = "nb")
plda <- predict(modlda, testing)
pnb <- predict(modnb, testing)
table(plda, pnb)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
training <- segmentationOriginal[segmentationOriginal$Case == "Train",]
testing <- segmentationOriginal[segmentationOriginal$Case == "Test",]
set.seed(125)
modCART <- train(Class ~ ., method = "rpart", data = training)
newdata <- data.frame(TotalIntench2 = 23,000, FiberWidthCh1 = 10, PerimStatusCh1=2)
predict(modCART,newdata)
newdata <- data.frame(TotalIntench2 = 23000, FiberWidthCh1 = 10, PerimStatusCh1=2)
predict(modCART,newdata)
newdata <- testing[which(testing$TotalIntench2 == 23000,
testing$FiberWidthCh1 == 10,
testing$PerimStatusCh1==2),]
predict(modCART,newdata)
newdata <- testing[which(testing$TotalIntench2 == 23000 &
testing$FiberWidthCh1 == 10 &
testing$PerimStatusCh1==2),]
predict(modCART,newdata)
[which(testing$TotalIntench2 == 23000 &
testing$FiberWidthCh1 == 10 &
testing$PerimStatusCh1==2)
predictions <- predict(modCART,newdata = testing)
testing$predictions <- predictions
names(testing)
head(testing$TotalIntenCh1)
library(dplyr)
names(testing)
library(dplyr)
select(testing,TotalIntenCh2 == 23000, FiberWidthCh1 == 10, PerimStatusCh1 == 2)
library(dplyr)
filter(testing,TotalIntenCh2 == 23000 & FiberWidthCh1 == 10 & PerimStatusCh1 == 2)
testing$FiberWidthCh1
library(dplyr)
filter(testing,TotalIntenCh2 == 23000 & FiberWidthCh1 == 10.00000 & PerimStatusCh1 == 2)
library(dplyr)
filter(testing,TotalIntenCh2 == 23000 )
library(dplyr)
which(testing$TotalIntenCh2 == 23000 )
predict(modCART,newdata = data.frame(TotalIntenCh2 = 23,000, FiberWidthCh1 = 10, PerimStatusCh1=2))
predict(modCART,newdata = data.frame(TotalIntenCh2 = 23000, FiberWidthCh1 = 10, PerimStatusCh1=2))
which(segmentationOriginal$FiberWidthCh1 == 10)
new.data.set = testing[0,]
View(new.data.set)
new.data.set = testing[NA,]
View(newdata)
View(new.data.set)
new.data.set = testing[0,]
new.data.set = testing[0,]
predict(modCART,testing(TotalIntenCh2 = 23000, FiberWidthCh1 = 10, PerimStatusCh1=2))
new.data.set = testing[0,]
predict(modCART,testing[,c(TotalIntenCh2 = 23000, FiberWidthCh1 = 10, PerimStatusCh1=2)])
summarize(testing$TotalIntenCh1)
summary(testing$TotalIntenCh1)
View(new.data.set)
new.data.set = testing[0,]
new.data.set$TotalIntenCh1 = 23000
new.data.set$FiberWidthCh1 = 10
new.data.set$PerimStatusCh1 =2
predict(modCART,new.data.set)
View(new.data.set)
new.data.set = testing[0,]
new.data.set$TotalIntenCh1 = 23000
new.data.set$FiberWidthCh1 = 10
new.data.set$PerimStatusCh1 =2
new.data.set = testing[0,]
new.data.set$TotalIntenCh1[1] = 23000
new.data.set$FiberWidthCh1[1] = 10
new.data.set$PerimStatusCh1[1] =2
predict(modCART,new.data.set)
new.data.set = testing[0,]
new.data.set$TotalIntenCh1[1] = 23000
new.data.set$FiberWidthCh1[1] = 10
new.data.set$PerimStatusCh1[1] =2
new.data.set = data.frame(NA); names(new.data.set) = names(testing)
new.data.set$TotalIntenCh1[1] = 23000
new.data.set$FiberWidthCh1[1] = 10
new.data.set$PerimStatusCh1[1] =2
View(new.data.set)
new.data.set = testing[1,]
new.data.set$TotalIntenCh1[1] = 23000
new.data.set$FiberWidthCh1[1] = 10
new.data.set$PerimStatusCh1[1] =2
View(new.data.set)
new.data.set = testing[1,]; new.data.set[1,] = NA
View(new.data.set)
new.data.set = testing[1,]; new.data.set[1,] = NA
new.data.set$TotalIntenCh1[1] = 23000
new.data.set$FiberWidthCh1[1] = 10
new.data.set$PerimStatusCh1[1] =2
predict(modCART,new.data.set)
new.data.set = testing[1,]; new.data.set[1,] = 0
new.data.set$TotalIntenCh1[1] = 23000
new.data.set$FiberWidthCh1[1] = 10
new.data.set$PerimStatusCh1[1] =2
predict(modCART,new.data.set)
plot(modCART$finalModel)
print(modCART$finalModel)
library(pgmm)
data(olive)
olive = olive[,-1]
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
View(olive)
modOlive <- train(Area ~ ., method = "rpart", data = olive)
newdata = as.data.frame(t(colMeans(olive)))
predict(modOlive,newdata)
summary(olive)
colMeans(olive)
t(colMeans(olive))
as.data.frame(t(colMeans(olive)))
newdata = as.data.frame(t(colMeans(olive)))
predict(modOlive,newdata)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
View(trainSA)
names(trainSA)
set.seed(13234)
modCHD <- train(factor(chd) ~ tobacco+ldl+adiposity+typea+obesity+alcohol+age, data = trainSA,
metgod = "glm", family = "binomial")
print(modCHD)
set.seed(13234)
modCHD <- train(factor(chd) ~ tobacco+ldl+adiposity+typea+obesity+alcohol+age, data = trainSA,
method = "glm", family = "binomial")
print(modCHD)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values
)/length(values)}
missClass(values = trainSA$chd,prediction = predict(modCHD,training,scale="response"))
missClass(values = trainSA$chd,prediction = predict(modCHD,trainSA,scale="response"))
missClass(values = trainSA$chd,prediction = modCHD$finalModel$fitted.values)
missClass(values = testSA$chd,prediction = modCHD$finalModel$fitted.values)
missClass(values = testSA$chd,predict(modCHD,testSA))
missClass(values = testSA$chd,predict(modCHD,testSA,type = "response"))
modCHD$finalModel$fitted.values
missClass(trainSA$chd,predict(modCHD,trainSA,type ="response"))
missClass(trainSA$chd,predict(modCHD,trainSA,type ="prob"))
missClass(trainSA$chd,predict(modCHD,trainSA,type ="raw"))
missClass(trainSA$chd,modCHD$finalModel$fitted.values)
missClass(values = testSA$chd,predict(modCHD,newdata = testSA,type = "prob"))
missClass(values = testSA$chd,predict(modCHD,newdata = testSA))
set.seed(13234)
modCHD <- train(chd ~ tobacco+ldl+adiposity+typea+obesity+alcohol+age, data = trainSA,
method = "glm", family = "binomial")
print(modCHD)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values
)/length(values)}
missClass(trainSA$chd,modCHD$finalModel$fitted.values)
missClass(values = testSA$chd,predict(modCHD,newdata = testSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
y
vovel.train
vowel.train
vowel.test
View(vowel.train)
vowel.test$y = factor(vowel.test$y)
vowel.train$y = factor(vowel.train$y)
View(vowel.train)
vowel.test$y = factor(vowel.test$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
modRF <- train(y ~ ., data = vowel.train, method = "rf")
print(modRF)
randomForest::importance(modRF)
randomForest::varImpPlot(modRF)
varImp(modRF)
randomForest::varImpPlot(modRF)
varImp(modRF)
plot(varImp(modRF))
randomForest()
randomForest(modRF)
vowel.test$y = factor(vowel.test$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
modRF <- randomForest(y ~ ., data = vowel.train)
print(modRF)
importance(modRF)
plot(varImp(modRF))
importance(modRF)
varImpPlot(modRF)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
library(caret)
vowel.test$y = factor(vowel.test$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
modRF <- randomForest(y ~ ., data = vowel.train)
print(modRF)
library(randomForest)
vowel.test$y = factor(vowel.test$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
modRF <- randomForest(y ~ ., data = vowel.train)
print(modRF)
importance(modRF)
varImpPlot(modRF)
vowel.test$y = factor(vowel.test$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
modRF <- train(y ~ ., data = vowel.train, method= "rf", trControl = trainControl(method = "cv"))
print(modRF)
varImp(modRF)
plot(varImp(modRF))
varImp.radomForest(modRF)
plot(varImp.radomForest(modRF))
varImp.randomForest(modRF)
plot(varImp.randomForest(modRF))
varImp.RandomForest(modRF)
plot(varImp.RandomForest(modRF))
library(party)
varImp.RandomForest(modRF)
plot(varImp.RandomForest(modRF))
varImp(modRF)
plot(varImp(modRF))
varImp(modRF,scale = FALSE)
plot(varImp(modRF,scale = FALSE))
vowel.test$y = factor(vowel.test$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
modRF <- randomForest(y ~ ., data = vowel.train)
print(modRF)
vowel.test$y = factor(vowel.test$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
modRF1 <- train(y ~ ., data = vowel.train, method= "rf", trControl = trainControl(method = "cv"))
print(modRF)
importance(modRF)
importance(modRF1)
varImp(modRF)
importance(modRF1)
plot(varImp(modRF),varImp(modRF1))
par(mfrow = c(1,2))
plot(varImp(modRF))
plot(varImp(modRF1))
par(mfrow = c(1,2));varImpPlot(modRF);plot(varImp(modRF1))
library(ElemStatLearn); data(prostate)
str(prostate)
training <- read.csv("train.csv")
final_testing <- read.csv("test.csv")
setwd("~/Desktop/2016/Data_science/Coursera_specialization/8_Practical_machine_learning/Project/Predictive_classification_of_human_activity_recognition_by_machine_learning/Predictive-classification-of-human-activity-recognition-by-machine-learning")
training <- read.csv("train.csv")
final_testing <- read.csv("test.csv")
library(caret);library(ggplot2); set.seed(23445)
INbuilding <- createDataPartition(y = training$classe,p=0.6,list = FALSE)
building <- training[INbuilding,]
rest <- training[-INbuilding,]
INtune <- createDataPartition(y = rest$classe,p=0.5,list = FALSE)
tune.testing <- rest[INtune,]
validation <- rest[-INtune,]
summary(building)
table(building$classe) # Classes are slightly unbalanced, we have more class A.
# Decide what to do with the missing values
apply(is.na(building),2,sum) #  11548 data points (98% of them) are consistently missing in
length(which(apply(is.na(building),2,sum)>0)) # 67 variables
plot(building$classe[is.na(building$amplitude_pitch_forearm)]) # slightly more NA's in A class but #overall balanced distribution across the classes
##################################################################################################
# STEP1: Drop the missing value containing variables as they are unlikely to add value to our #prediction (process the tune.testing, validation, final_testing sets exactly in the same way )
tune.testing <- tune.testing[,-which(apply(is.na(building),2,sum)>0)]
validation <- validation[,-which(apply(is.na(building),2,sum)>0)]
final_testing <- final_testing[,-which(apply(is.na(building),2,sum)>0)]
building <- building[,-which(apply(is.na(building),2,sum)>0)]
##################################################################################################
# Find and remove near-zero variance variables
nsv <- nearZeroVar(x = building, saveMetrics = TRUE)
sum(!nsv$nzv) # 59 variables have non-zero variance and will be kept in the building set
##################################################################################################
# STEP 2: Remove nzv variables, (process the tune.testing, validation and final_testing sets exactly in the same way #)
tune.testing <- tune.testing[,!nsv$nzv]
validation <- validation[,!nsv$nzv]
final_testing <- final_testing[,!nsv$nzv]
building <- building[,!nsv$nzv]
##################################################################################################
#
#                                        EDA
#
##################################################################################################
# Next, let's have a look at the correlation between the continuous variables
cont <- !sapply(building,is.factor) # Continuous variables in the building set
M <- abs(cor(building[,cont])) # M is an absolute value correlation matrix representing the pairwise #correlations between all continuous variables
diag(M) <- 0 # We replace the diagonal values with zero (just because these are the correations with  #themselves we are not interested in capturing them).
which(M > 0.8, arr.ind = TRUE) # What are the highest correated variables?
unique(row.names(which(M > 0.8, arr.ind = TRUE))) # We find that there are 18 highly correlated #predictiors in the data set
qplot(building[,row.names(M)[7]],building[,colnames(M)[5]], color = classe, data = building)
qplot(building[,row.names(M)[8]],building[,colnames(M)[5]], color = classe, data = building)
plot(building[,row.names(M)[13]],building[,colnames(M)[5]])
plot(building[,row.names(M)[15]],building[,colnames(M)[6]])
plot(building[,row.names(M)[33]],building[,colnames(M)[40]])
qplot(building[,row.names(M)[33]],building[,colnames(M)[40]], color = classe, data = building)
cor.variables <- building[,unique(row.names(which(M > 0.8, arr.ind = TRUE)))]
cor.variables$classe <- building$classe
# Performing PCA to see if dimension reduction might help to resolve classes
prePCA <- preProcess(cor.variables[,-19],method = "pca")
PCAcor <- predict(prePCA,cor.variables[,-19])
qplot(PCAcor$PC1,PCAcor$PC2, color = classe, data = cor.variables)
qplot(PCAcor$PC1,PCAcor$PC3, color = classe, data = cor.variables)
qplot(PCAcor$PC2,PCAcor$PC3, color = classe, data = cor.variables)
# Not much obvious advantage gained by calculating principal components to seperate classes
# We notice that the correlated predictors have already clusters within them. In this case, I will not #attempt #further dimension reduction, because the data looks like suitable for classification algorithms (rather than linear #models)
##################################################################################################
# STEP3: Converting factor variables into dummy variables
which(sapply(building[,-59],is.factor)) # Only two factor variables remained in the datasets
# Note that one of them is a time stamp composed of 20 unique values. At this point we will model them as categorical variables
factors <- which(sapply(building[,-59],is.factor))
# Convert these variables into dummy variables:
dummies <- dummyVars(classe ~ user_name + cvtd_timestamp, data = building)
# Add them into building and all test and validation sets and drop the original factor variables
building <- cbind(building[,-factors],predict(dummies,building))
tune.testing <- cbind(tune.testing[,-factors],predict(dummies,tune.testing))
validation <- cbind(validation[,-factors],predict(dummies,validation))
names(final_testing)[59] <- "classe" # Names in newdata should match the object to use predict
final_testing <- cbind(final_testing[,-factors],predict(dummies,final_testing))
##################################################################################################
# STEP4: Eliminating the bias from the indexing variable X
# We notice that the variable X is just an indexing variable that can perfectly seperate the classes
qplot(X,classe,data = building,color = classe)+theme_bw()
# Such a variable has no real predictive power because the real samples will not be ordered based on such index
# Therefore, we remove the variable X from all sets before building the models:
building <- subset(building,select = -c(X))
tune.testing <- subset(tune.testing,select = -c(X))
validation <- subset(validation,select = -c(X))
final_testing <- subset(final_testing,select = -c(X))
##########################################################################
# STEP5 :Finally legitimize the names of the variables: (needed for the classification algorithms to run #smoothly)
names(building) <- make.names(names(building))
names(tune.testing) <- make.names(names(tune.testing))
names(validation) <- make.names(names(validation))
names(final_testing) <- make.names(names(final_testing))
RPART = readRDS("RPART.rds")
RF = readRDS("RF.rds")
GBM = readRDS("GBM.rds")
SVM = readRDS("SVM.rds")
# Using resamples function from the caret package to summarize the data
modelsummary <- resamples(list(RPART=RPART,RF=RF,GBM=GBM,SVM=SVM))
# In-sample accuracy values for each model
summary(modelsummary)$statistics$Accuracy
# If necessary we can also produce 'variable importance' plots to see which variables are making most contributions #to each model
dotPlot(varImp(RPART))
dotPlot(varImp(RF))
dotPlot(varImp(SVM))
dotPlot(varImp(GBM))
pred.tune.RPART <- predict(RPART,newdata = tune.testing)
pred.tune.RF <- predict(RF,newdata = tune.testing)
pred.tune.SVM <- predict(SVM,newdata = tune.testing)
pred.tune.GBM <- predict(GBM,newdata = tune.testing)
# Collect accuracy values in a data.frame:
tune.testing.predictions <-data.frame(pred.tune.RPART,pred.tune.RF,pred.tune.SVM,pred.tune.GBM)
tune.testing.accuracy <- apply(tune.testing.predictions, 2, function(x){
temp=confusionMatrix(x,tune.testing$classe)$overall[1]
names(temp) <- names(x)
return(temp)
})
tune.testing.predictions$classe <- tune.testing$classe
# Note evaluated models below yet:
stacked.model.rpart <- train(classe ~ ., method = "rpart", data = tune.testing.predictions,trControl = trainControl(method = "cv", number = 10) )
stacked.model.rf <- train(classe ~ ., method = "rf", data = tune.testing.predictions,trControl = trainControl(method = "cv", number = 10) )
stacked.model.svm <- train(classe ~ ., method = "svm", data = tune.testing.predictions,trControl = trainControl(method = "cv", number = 10) )
stacked.model.gbm <- train(classe ~ ., method = "gbm", data = tune.testing.predictions,trControl = trainControl(method = "cv", number = 10) )
stacked.model.rpart
stacked.model.rf
stacked.model.svm
stacked.model.svm <- train(classe ~ ., method = "svmRadial", data = tune.testing.predictions,trControl = trainControl(method = "cv", number = 10) )
stacked.model.svm
stacked.model.gbm
stacked.modelsummary <- resamples(list(RPART=stacked.model.rpart,RF=stacked.model.rf,GBM=stacked.model.gbm ,SVM=stacked.model.svm))
# In-sample accuracy values for each model
summary(stacked.modelsummary)$statistics$Accuracy
tune.testing.accuracy
# Individual model fits
pred.validation.RPART <- predict(RPART,newdata = validation)
pred.validation.RF <- predict(RF,newdata = validation)
pred.validation.SVM <- predict(SVM,newdata = validation)
pred.validation.GBM <- predict(GBM,newdata = validation)
# Stacked model fits
pred.validation.Stacked.RPART <- predict(stacked.model.rpart,newdata = validation)
pred.validation.Stacked.RF <- predict(stacked.model.rf,newdata = validation)
pred.validation.Stacked.SVM <- predict(stacked.model.svm,newdata = validation)
pred.validation.Stacked.GBM <- predict(stacked.model.gbm,newdata = validation)
# Collect accuracy values in a data.frame:
validation.predictions <-data.frame(pred.validation.RPART,pred.validation.RF,pred.validation.SVM,pred.validation.GBM,pred.validation.Stacked.RPART,pred.validation.Stacked.RF,pred.validation.Stacked.SVM,pred.validation.Stacked.GBM)
validation.accuracy <- apply(validation.predictions, 2, function(x){
temp=confusionMatrix(x,validation$classe)$overall[1]
names(temp) <- names(x)
return(temp)
})
validation.accuracy
dotPlot(varImp(pred.validation.Stacked.RF))
dotPlot(varImp(stacked.model.rf))
dotPlot(varImp(stacked.model.gbm))
dotPlot(varImp(stacked.model.svm))
dotPlot(varImp(stacked.model.rpart))
