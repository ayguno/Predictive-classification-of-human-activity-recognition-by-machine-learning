predict(modCART,new.data.set)
new.data.set = testing[1,]; new.data.set[1,] = 0
new.data.set$TotalIntenCh1[1] = 23000
new.data.set$FiberWidthCh1[1] = 10
new.data.set$PerimStatusCh1[1] =2
predict(modCART,new.data.set)
plot(modCART$finalModel)
print(modCART$finalModel)
library(pgmm)
data(olive)
olive = olive[,-1]
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
View(olive)
modOlive <- train(Area ~ ., method = "rpart", data = olive)
newdata = as.data.frame(t(colMeans(olive)))
predict(modOlive,newdata)
summary(olive)
colMeans(olive)
t(colMeans(olive))
as.data.frame(t(colMeans(olive)))
newdata = as.data.frame(t(colMeans(olive)))
predict(modOlive,newdata)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
View(trainSA)
names(trainSA)
set.seed(13234)
modCHD <- train(factor(chd) ~ tobacco+ldl+adiposity+typea+obesity+alcohol+age, data = trainSA,
metgod = "glm", family = "binomial")
print(modCHD)
set.seed(13234)
modCHD <- train(factor(chd) ~ tobacco+ldl+adiposity+typea+obesity+alcohol+age, data = trainSA,
method = "glm", family = "binomial")
print(modCHD)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values
)/length(values)}
missClass(values = trainSA$chd,prediction = predict(modCHD,training,scale="response"))
missClass(values = trainSA$chd,prediction = predict(modCHD,trainSA,scale="response"))
missClass(values = trainSA$chd,prediction = modCHD$finalModel$fitted.values)
missClass(values = testSA$chd,prediction = modCHD$finalModel$fitted.values)
missClass(values = testSA$chd,predict(modCHD,testSA))
missClass(values = testSA$chd,predict(modCHD,testSA,type = "response"))
modCHD$finalModel$fitted.values
missClass(trainSA$chd,predict(modCHD,trainSA,type ="response"))
missClass(trainSA$chd,predict(modCHD,trainSA,type ="prob"))
missClass(trainSA$chd,predict(modCHD,trainSA,type ="raw"))
missClass(trainSA$chd,modCHD$finalModel$fitted.values)
missClass(values = testSA$chd,predict(modCHD,newdata = testSA,type = "prob"))
missClass(values = testSA$chd,predict(modCHD,newdata = testSA))
set.seed(13234)
modCHD <- train(chd ~ tobacco+ldl+adiposity+typea+obesity+alcohol+age, data = trainSA,
method = "glm", family = "binomial")
print(modCHD)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values
)/length(values)}
missClass(trainSA$chd,modCHD$finalModel$fitted.values)
missClass(values = testSA$chd,predict(modCHD,newdata = testSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
y
vovel.train
vowel.train
vowel.test
View(vowel.train)
vowel.test$y = factor(vowel.test$y)
vowel.train$y = factor(vowel.train$y)
View(vowel.train)
vowel.test$y = factor(vowel.test$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
modRF <- train(y ~ ., data = vowel.train, method = "rf")
print(modRF)
randomForest::importance(modRF)
randomForest::varImpPlot(modRF)
varImp(modRF)
randomForest::varImpPlot(modRF)
varImp(modRF)
plot(varImp(modRF))
randomForest()
randomForest(modRF)
vowel.test$y = factor(vowel.test$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
modRF <- randomForest(y ~ ., data = vowel.train)
print(modRF)
importance(modRF)
plot(varImp(modRF))
importance(modRF)
varImpPlot(modRF)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
library(caret)
vowel.test$y = factor(vowel.test$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
modRF <- randomForest(y ~ ., data = vowel.train)
print(modRF)
library(randomForest)
vowel.test$y = factor(vowel.test$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
modRF <- randomForest(y ~ ., data = vowel.train)
print(modRF)
importance(modRF)
varImpPlot(modRF)
vowel.test$y = factor(vowel.test$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
modRF <- train(y ~ ., data = vowel.train, method= "rf", trControl = trainControl(method = "cv"))
print(modRF)
varImp(modRF)
plot(varImp(modRF))
varImp.radomForest(modRF)
plot(varImp.radomForest(modRF))
varImp.randomForest(modRF)
plot(varImp.randomForest(modRF))
varImp.RandomForest(modRF)
plot(varImp.RandomForest(modRF))
library(party)
varImp.RandomForest(modRF)
plot(varImp.RandomForest(modRF))
varImp(modRF)
plot(varImp(modRF))
varImp(modRF,scale = FALSE)
plot(varImp(modRF,scale = FALSE))
vowel.test$y = factor(vowel.test$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
modRF <- randomForest(y ~ ., data = vowel.train)
print(modRF)
vowel.test$y = factor(vowel.test$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
modRF1 <- train(y ~ ., data = vowel.train, method= "rf", trControl = trainControl(method = "cv"))
print(modRF)
importance(modRF)
importance(modRF1)
varImp(modRF)
importance(modRF1)
plot(varImp(modRF),varImp(modRF1))
par(mfrow = c(1,2))
plot(varImp(modRF))
plot(varImp(modRF1))
par(mfrow = c(1,2));varImpPlot(modRF);plot(varImp(modRF1))
library(ElemStatLearn); data(prostate)
str(prostate)
setwd("~/Desktop/2016/Harvard Catalyst/Week24")
dental = read.csv("dental.csv")
View(dental)
View(dental)
dental_long <- reshape(dental,varying = list(c("y1","y2","y3","y4")), idvar = "id",
v.names = "y",timevar = "obs",direction = "long")
View(dental_long)
dental_long <- reshape(dental,varying = list(c("y1","y2","y3","y4")), idvar = "id",
v.names = "y",timevar = "obs",direction = "long")
dental_long <- dental_long[order(dental_long$id,detal_long$obs),]
dental_long <- reshape(dental,varying = list(c("y1","y2","y3","y4")), idvar = "id",
v.names = "y",timevar = "obs",direction = "long")
dental_long <- dental_long[order(dental_long$id,dental_long$obs),]
View(dental_long)
dental_long$obsf<-factor(dental_long$obs)
dental_long$genderf<-factor(dental_long$gender)
View(dental_long)
library(dplyr)
library(dplyr)
summary_dental_long <- dental_long %>% group_by(genderf,obsf) %>% summarise(mean_dist = mean(y))
View(summary_dental_long)
library(ggplot2)
library(dplyr); library(ggplot2)
summary_dental_long <- dental_long %>% group_by(genderf,obsf) %>% summarise(mean_dist = mean(y))
qplot(obsf,mean_dist, data = summary_dental_long, color = genderf)+geom_line()+theme_bw()
plot(summary_dental_long$obsf,summary_dental_long$mean_dist)
plot(summary_dental_long$obsf,summary_dental_long$mean_dist, type="l")
library(dplyr); library(ggplot2)
summary_dental_long <- dental_long %>% group_by(genderf,obsf) %>% summarise(mean_dist = mean(y))
ggplot(aes(obsf,mean_dist), data = summary_dental_long, color = genderf)+
geom_line()+theme_bw()
library(dplyr); library(ggplot2)
summary_dental_long <- dental_long %>% group_by(genderf,obsf) %>% summarise(mean_dist = mean(y))
ggplot(aes(obsf,mean_dist), data = summary_dental_long, color = genderf)+
geom_line(aes(obsf,mean_dist))+theme_bw()
library(dplyr); library(ggplot2)
summary_dental_long <- dental_long %>% group_by(genderf,obsf) %>% summarise(mean_dist = mean(y))
ggplot(aes(obsf,mean_dist,color = genderf), data = summary_dental_long)+
geom_line(aes(obsf,mean_dist))+theme_bw()
library(dplyr); library(ggplot2)
summary_dental_long <- dental_long %>% group_by(genderf,obsf) %>% summarise(mean_dist = mean(y))
ggplot(aes(obsf,mean_dist,color = genderf), data = summary_dental_long)+
geom_line()+theme_bw()
library(dplyr); library(ggplot2)
summary_dental_long <- dental_long %>% group_by(genderf,obsf) %>% summarise(mean_dist = mean(y))
ggplot(aes(obsf,mean_dist,color = genderf), data = summary_dental_long)+
geom_point()+
geom_line()+theme_bw()
library(dplyr); library(ggplot2)
summary_dental_long <- dental_long %>% group_by(genderf,obsf) %>% summarise(mean_dist = mean(y))
ggplot(aes(as.numeric(obsf),mean_dist,color = genderf), data = summary_dental_long)+
geom_point()+
geom_line()+theme_bw()
library(dplyr); library(ggplot2)
summary_dental_long <- dental_long %>% group_by(genderf,obsf) %>% summarise(mean_dist = mean(y))
ggplot(aes(as.numeric(obsf),mean_dist,color = genderf), data = summary_dental_long)+
geom_line()+theme_bw()
library(dplyr); library(ggplot2)
summary_dental_long <- dental_long %>% group_by(genderf,obsf) %>% summarise(mean_dist = mean(y))
ggplot(aes(as.numeric(obsf),mean_dist,color = genderf), data = summary_dental_long)+
geom_line()+theme_bw()+scale_color_manual(values = c("red","navy"))
library(dplyr); library(ggplot2)
summary_dental_long <- dental_long %>% group_by(genderf,obsf) %>% summarise(mean_dist = mean(y))
ggplot(aes(as.numeric(obsf),mean_dist,color = genderf), data = summary_dental_long)+
geom_line(size =2)+theme_bw()+scale_color_manual(values = c("red","navy"))
library(dplyr); library(ggplot2)
summary_dental_long <- dental_long %>% group_by(genderf,obsf) %>% summarise(mean_dist = mean(y))
ggplot(aes(as.numeric(obsf),mean_dist,color = genderf), data = summary_dental_long)+
geom_line(size =1)+theme_bw()+scale_color_manual(values = c("red","navy"))
library(nlme)
library(nlme)
summary(gls(y ~ genderf + obsf + gender * obsf, data = dental_long, correlation = corSymm(form = ~1|id, weights = varIdent(form = ~1|obsf))))
library(nlme)
summary(gls(y ~ genderf + obsf + gender * obsf, data = dental_long, correlation = corSymm(form = ~1|id), weights = varIdent(form = ~1|obsf))))
library(nlme)
summary(gls(y ~ genderf + obsf + gender * obsf, data = dental_long, correlation = corSymm(form = ~1|id), weights = varIdent(form = ~1|obsf)))
library(nlme)
summary(gls(y~ genderf+obsf+genderf:obsf, data=dental_long, correlation=corSymm(form=~1|id), weights=varIdent(form=~1|obsf)))
library(car)
linearHypothesis(gls(y~ genderf+obsf+genderf:obsf, data=dental_long, correlation=corSymm(form=~1|id),weights=varIdent(form=~1|obsf)),c("genderf1:obsf2=0","genderf1:obsf3=0","genderf1:obsf4=0"))
View(dental_long)
dental_long$ageyr <-8+(dental_long$obs-1)*2
View(dental_long)
summary(gls(y~ genderf+ageyr+genderf:ageyr, data=dental_long, correlation=corSymm(form=~1|id),weights=varIdent(form=~1|obsf)))
dental_long$ageyrsq <-dental_long$ageyr^2
summary(gls(y~ genderf+ageyr+genderf:ageyr+ageyrsq+genderf:ageyrsq, data=dental_long, correlation=corSymm(form=~1|id),weights=varIdent(form=~1|obsf)))
linear.hypothesis(summary(gls(y~ genderf+ageyr+genderf:ageyr+ageyrsq+genderf:ageyrsq, data=dental_long, correlation=corSymm(form=~1|id),weights=varIdent(form=~1|obsf))))
linearHypothesis(summary(gls(y~ genderf+ageyr+genderf:ageyr+ageyrsq+genderf:ageyrsq, data=dental_long, correlation=corSymm(form=~1|id),weights=varIdent(form=~1|obsf))))
linearHypothesis(gls(y~ genderf+ageyr+genderf:ageyr+ageyrsq+genderf:ageyrsq, data=dental_long, correlation=corSymm(form=~1|id),weights=varIdent(form=~1|obsf)), c("ageyrsq=0","genderf1:ageyrsq=0"))
dir()
weight_loss <- read.csv("wtloss.csv")
View(weight_loss)
weight_loss <- read.csv("wtloss.csv")
weight_loss_long <- reshape(weight_loss,varying = list(c("y1","y2","y3","y4")), idvar = "id",
v.names = "y",timevar = "obs",direction = "long")
View(weight_loss_long)
weight_loss <- read.csv("wtloss.csv")
weight_loss_long <- reshape(weight_loss,varying = list(c("y1","y2","y3","y4")), idvar = "id",
v.names = "y",timevar = "obs",direction = "long")
weight_loss_long <- weight_loss_long[order(weight_loss_long$id,weight_loss_long$obs),]
View(weight_loss_long)
View(dental_long)
weight_loss <- read.csv("wtloss.csv")
weight_loss_long <- reshape(weight_loss,varying = list(c("y1","y2","y3","y4")), idvar = "id",
v.names = "y",timevar = "obs",direction = "long")
weight_loss_long <- weight_loss_long[order(weight_loss_long$id,weight_loss_long$obs),]
weight_loss_long$programf <- factor(weight_loss_long$program)
weight_loss_long$obsf <- factor(weight_loss_long$program)
View(weight_loss_long)
weight_loss <- read.csv("wtloss.csv")
weight_loss_long <- reshape(weight_loss,varying = list(c("y1","y2","y3","y4")), idvar = "id",
v.names = "y",timevar = "obs",direction = "long")
weight_loss_long <- weight_loss_long[order(weight_loss_long$id,weight_loss_long$obs),]
weight_loss_long$programf <- factor(weight_loss_long$program)
weight_loss_long$obsf <- factor(weight_loss_long$obs)
View(weight_loss_long)
summary(gls(y~ programf+obsf+programf:obsf, data=weight_loss_long correlation=corSymm(form=~1|id), weights=varIdent(form=~1|obsf)))
summary(gls(y~ programf+obsf+programf:obsf, data=weight_loss_long, correlation=corSymm(form=~1|id), weights=varIdent(form=~1|obsf)))
weight_loss <- read.csv("wtloss.csv")
weight_loss_long <- reshape(weight_loss,varying = list(c("y1","y2","y3","y4")), idvar = "id",
v.names = "y",timevar = "obs",direction = "long")
weight_loss_long <- weight_loss_long[order(weight_loss_long$id,weight_loss_long$obs),]
weight_loss_long$programf <- factor(weight_loss_long$program)
weight_loss_long$obsf <- factor(weight_loss_long$obs)
summary(gls(y~ programf+obsf+programf:obsf, data=weight_loss_long, correlation=corSymm(form=~1|id), weights=varIdent(form=~1|obsf)))
linearHypothesis(gls(y~ programf+obsf+programf:obsf, data=weight_loss_long, correlation=corSymm(form=~1|id), weights=varIdent(form=~1|obsf)),c("programf2:obsf2=0","programf2:obsf3=0","programf2:obsf4=0"))
setwd("~/Desktop/2016/Data_science/Coursera_specialization/8_Practical_machine_learning/Project/Predictive_classification_of_human_activity_recognition_by_machine_learning/Predictive-classification-of-human-activity-recognition-by-machine-learning")
training <- read.csv("train.csv")
final_testing <- read.csv("test.csv")
library(caret);library(ggplot2); set.seed(23445)
INbuilding <- createDataPartition(y = training$classe,p=0.6,list = FALSE)
building <- training[INbuilding,]
rest <- training[-INbuilding,]
INtune <- createDataPartition(y = rest$classe,p=0.5,list = FALSE)
tune.testing <- rest[INtune,]
validation <- rest[-INtune,]
apply(is.na(building),2,sum) #  11548 data points (98% of them) are consistently missing in
length(which(apply(is.na(building),2,sum)>0)) # 67 variables
plot(building$classe[is.na(building$amplitude_pitch_forearm)]) # slightly more NA's in A class but #overall balanced distribution across the classes
##########################################
# STEP1: Drop the missing value containing variables as they are unlikely to add value to our #prediction (process the tune.testing, validation, final_testing sets exactly in the same way )
tune.testing <- tune.testing[,-which(apply(is.na(building),2,sum)>0)]
validation <- validation[,-which(apply(is.na(building),2,sum)>0)]
final_testing <- final_testing[,-which(apply(is.na(building),2,sum)>0)]
building <- building[,-which(apply(is.na(building),2,sum)>0)]
nsv <- nearZeroVar(x = building, saveMetrics = TRUE)
sum(!nsv$nzv) # 59 variables have non-zero variance and will be kept in the building set
tune.testing <- tune.testing[,!nsv$nzv]
validation <- validation[,!nsv$nzv]
final_testing <- final_testing[,!nsv$nzv]
building <- building[,!nsv$nzv]
cont <- !sapply(building,is.factor) # Continuous variables in the building set
M <- abs(cor(building[,cont])) # M is an absolute value correlation matrix representing the pairwise #correlations between all continuous variables
diag(M) <- 0 # We replace the diagonal values with zero (just because these are the correations with  #themselves we are not interested in capturing them).
which(M > 0.8, arr.ind = TRUE) # What are the highest correated variables?
unique(row.names(which(M > 0.8, arr.ind = TRUE))) # We find that there are 18 highly correlated #predictiors in the data set
qplot(building[,row.names(M)[7]],building[,colnames(M)[5]], color = classe, data = building)
qplot(building[,row.names(M)[8]],building[,colnames(M)[5]], color = classe, data = building)
plot(building[,row.names(M)[13]],building[,colnames(M)[5]])
plot(building[,row.names(M)[15]],building[,colnames(M)[6]])
plot(building[,row.names(M)[33]],building[,colnames(M)[40]])
qplot(building[,row.names(M)[33]],building[,colnames(M)[40]], color = classe, data = building)
cor.variables <- building[,unique(row.names(which(M > 0.8, arr.ind = TRUE)))]
cor.variables$classe <- building$classe
prePCA <- preProcess(log10(cor.variables[,-19]+1),method = c("BoxCox","pca"))
prePCA <- preProcess(cor.variables[,-19],method = c("BoxCox","pca"))
PCAcor <- predict(prePCA,cor.variables[,-19])
View(PCAcor)
qplot(PC1,PC2, color = classe, data = cor.variables)
qplot(PCAcor$PC1,PCAcor$PC2, color = classe, data = cor.variables)
qplot(PCAcor$PC1,PCAcor$PC3, color = classe, data = cor.variables)
qplot(PCAcor$PC2,PCAcor$PC3, color = classe, data = cor.variables)
prePCA <- preProcess(cor.variables[,-19],method = "pca")
PCAcor <- predict(prePCA,cor.variables[,-19])
qplot(PCAcor$PC1,PCAcor$PC2, color = classe, data = cor.variables)
qplot(PCAcor$PC1,PCAcor$PC3, color = classe, data = cor.variables)
qplot(PCAcor$PC2,PCAcor$PC3, color = classe, data = cor.variables)
which(sapply(building[,-59],is.factor)) # Only two factor variables remained in the datasets
# Note that one of them is a time stamp composed of 20 unique values. At this point we will model them as categorical variables
factors <- which(sapply(building[,-59],is.factor))
# Convert these variables into dummy variables:
dummies <- dummyVars(classe ~ user_name + cvtd_timestamp, data = building)
building <- cbind(building[,-factors],predict(dummies,building))
tune.testing <- cbind(tune.testing[,-factors],predict(dummies,tune.testing))
validation <- cbind(validation[,-factors],predict(dummies,validation))
final_testing <- cbind(final_testing[,-factors],predict(dummies,final_testing))
training <- read.csv("train.csv")
final_testing <- read.csv("test.csv")
training <- read.csv("train.csv")
final_testing <- read.csv("test.csv")
library(caret);library(ggplot2); set.seed(23445)
INbuilding <- createDataPartition(y = training$classe,p=0.6,list = FALSE)
building <- training[INbuilding,]
rest <- training[-INbuilding,]
INtune <- createDataPartition(y = rest$classe,p=0.5,list = FALSE)
tune.testing <- rest[INtune,]
validation <- rest[-INtune,]
# Decide what to do with the missing values
apply(is.na(building),2,sum) #  11548 data points (98% of them) are consistently missing in
length(which(apply(is.na(building),2,sum)>0)) # 67 variables
plot(building$classe[is.na(building$amplitude_pitch_forearm)]) # slightly more NA's in A class but #overall balanced distribution across the classes
##########################################
# STEP1: Drop the missing value containing variables as they are unlikely to add value to our #prediction (process the tune.testing, validation, final_testing sets exactly in the same way )
tune.testing <- tune.testing[,-which(apply(is.na(building),2,sum)>0)]
validation <- validation[,-which(apply(is.na(building),2,sum)>0)]
final_testing <- final_testing[,-which(apply(is.na(building),2,sum)>0)]
building <- building[,-which(apply(is.na(building),2,sum)>0)]
Find and remove near-zero variance variables
nsv <- nearZeroVar(x = building, saveMetrics = TRUE)
sum(!nsv$nzv) # 59 variables have non-zero variance and will be kept in the building set
##########################################
# STEP 2: Remove nzv variables, (process the tune.testing, validation and final_testing sets exactly in the same way #)
tune.testing <- tune.testing[,!nsv$nzv]
validation <- validation[,!nsv$nzv]
final_testing <- final_testing[,!nsv$nzv]
building <- building[,!nsv$nzv]
###########################################
which(sapply(building[,-59],is.factor)) # Only two factor variables remained in the datasets
# N
factors <- which(sapply(building[,-59],is.factor))
dummies <- dummyVars(classe ~ user_name + cvtd_timestamp, data = building)
building <- cbind(building[,-factors],predict(dummies,building))
tune.testing <- cbind(tune.testing[,-factors],predict(dummies,tune.testing))
validation <- cbind(validation[,-factors],predict(dummies,validation))
final_testing[,-factors]
predict(dummies,final_testing)
names(final_testing)
predict(dummies,final_testing[,-1])
predict(dummies,final_testing[,-59])
final_testing[,c(2,5)]
names(final_testing[,c(2,5)])
identical(names(final_testing[,c(2,5)]),names(which(sapply(building[,-59],is.factor)))
)
names(which(sapply(building[,-59],is.factor)))
training <- read.csv("train.csv")
final_testing <- read.csv("test.csv")
identical(names(final_testing[,-160]),names(training[,-160]))
tune.testing <- tune.testing[,-which(apply(is.na(building),2,sum)>0)]
validation <- validation[,-which(apply(is.na(building),2,sum)>0)]
final_testing <- final_testing[,-which(apply(is.na(building),2,sum)>0)]
building <- building[,-which(apply(is.na(building),2,sum)>0)]
##########################################
# Find and remove near-zero variance variables
nsv <- nearZeroVar(x = building, saveMetrics = TRUE)
sum(!nsv$nzv) # 59 variables have non-zero variance and will be kept in the building set
##########################################
# STEP 2: Remove nzv variables, (process the tune.testing, validation and final_testing sets exactly in the same way #)
tune.testing <- tune.testing[,!nsv$nzv]
validation <- validation[,!nsv$nzv]
final_testing <- final_testing[,!nsv$nzv]
building <- building[,!nsv$nzv]
###########################################
library(caret);library(ggplot2); set.seed(23445)
INbuilding <- createDataPartition(y = training$classe,p=0.6,list = FALSE)
building <- training[INbuilding,]
rest <- training[-INbuilding,]
INtune <- createDataPartition(y = rest$classe,p=0.5,list = FALSE)
tune.testing <- rest[INtune,]
validation <- rest[-INtune,]
tune.testing <- tune.testing[,-which(apply(is.na(building),2,sum)>0)]
validation <- validation[,-which(apply(is.na(building),2,sum)>0)]
final_testing <- final_testing[,-which(apply(is.na(building),2,sum)>0)]
building <- building[,-which(apply(is.na(building),2,sum)>0)]
##########################################
# Find and remove near-zero variance variables
nsv <- nearZeroVar(x = building, saveMetrics = TRUE)
sum(!nsv$nzv) # 59 variables have non-zero variance and will be kept in the building set
##########################################
# STEP 2: Remove nzv variables, (process the tune.testing, validation and final_testing sets exactly in the same way #)
tune.testing <- tune.testing[,!nsv$nzv]
validation <- validation[,!nsv$nzv]
final_testing <- final_testing[,!nsv$nzv]
building <- building[,!nsv$nzv]
###########################################
which(sapply(building[,-59],is.factor))
factors <- which(sapply(building[,-59],is.factor))
dummies <- dummyVars(classe ~ user_name + cvtd_timestamp, data = building)
final_testing <- cbind(final_testing[,-factors],predict(dummies,final_testing))
predict(dummies,final_testing)
final_testing <- final_testing %>% rename(problem_id,classe)
names(final_testing)[20] <- "classe"
training <- read.csv("train.csv")
final_testing <- read.csv("test.csv")
nameframe <- data.frame(names(training),names(final_testing))
library(caret);library(ggplot2); set.seed(23445)
INbuilding <- createDataPartition(y = training$classe,p=0.6,list = FALSE)
building <- training[INbuilding,]
rest <- training[-INbuilding,]
INtune <- createDataPartition(y = rest$classe,p=0.5,list = FALSE)
tune.testing <- rest[INtune,]
validation <- rest[-INtune,]
tune.testing <- tune.testing[,-which(apply(is.na(building),2,sum)>0)]
validation <- validation[,-which(apply(is.na(building),2,sum)>0)]
final_testing <- final_testing[,-which(apply(is.na(building),2,sum)>0)]
building <- building[,-which(apply(is.na(building),2,sum)>0)]
# Find and remove near-zero variance variables
nsv <- nearZeroVar(x = building, saveMetrics = TRUE)
sum(!nsv$nzv) # 59 variables have non-zero variance and will be kept in the building set
##########################################
# STEP 2: Remove nzv variables, (process the tune.testing, validation and final_testing sets exactly in the same way #)
tune.testing <- tune.testing[,!nsv$nzv]
validation <- validation[,!nsv$nzv]
final_testing <- final_testing[,!nsv$nzv]
building <- building[,!nsv$nzv]
###########################################
which(sapply(building[,-59],is.factor)) # Only two factor variables remained in the datasets
# Note that one of them is a time stamp composed of 20 unique values. At this point we will model them as categorical variables
factors <- which(sapply(building[,-59],is.factor))
# Convert these variables into dummy variables:
dummies <- dummyVars(classe ~ user_name + cvtd_timestamp, data = building)
# Add them into building and all test and validation sets and drop the original factor variables
names(final_testing)[59] <- "classe"
final_testing <- cbind(final_testing[,-factors],predict(dummies,final_testing))
building <- cbind(building[,-factors],predict(dummies,building))
tune.testing <- cbind(tune.testing[,-factors],predict(dummies,tune.testing))
validation <- cbind(validation[,-factors],predict(dummies,validation))
factorial(4)
install.packages("combinat")
library(combinat)
library(combinat)
lsmod = list ("rpart","gbm","rf","svm")
lsmod.per = permn(lsmod)
lsmod.per
lsmod.per[1]
lsmod.per[2]
library(combinat)
lsmod = list ("rpart","gbm","rf","svm")
lsmod.per = unlist(permn(lsmod))
library(combinat)
lsmod = list ("rpart","gbm","rf","svm")
lsmod.per = permn(lsmod)
data.frame(lsmod.per[1],lsmod.per[2])
data.frame(lsmod.per[1][[1]],lsmod.per[2][[1]])
data.frame(lsmod.per[[1]],lsmod.per[[2]])
library(combinat)
lsmod = list ("rpart","gbm","rf","svm")
lsmod.combin = combn(lsmod)
library(combinat)
lsmod = list ("rpart","gbm","rf","svm")
lsmod.combin = combn(lsmod,1)
library(combinat)
lsmod = list ("rpart","gbm","rf","svm")
lsmod.combin = list(combn(lsmod,1),combn(lsmod,2),combn(lsmod,3),combn(lsmod,4))
combn(lsmod,2)
combn(lsmod,3)
library(combinat)
lsmod = list ("rpart","gbm","rf","svm")
lsmod.combin = list(combn(lsmod,1, simplify = FALSE),combn(lsmod,2,simplify = FALSE),
combn(lsmod,3,simplify = FALSE),combn(lsmod,4,simplify = FALSE))
unlist(lsmod.combin)
library(combinat)
lsmod = list ("rpart","gbm","rf","svm")
lsmod.combin = list(combn(lsmod,1),combn(lsmod,2),combn(lsmod,3),combn(lsmod,4))
library(combinat)
lsmod = list ("rpart","gbm","rf","svm")
lsmod.combin = list(combn(lsmod,2),combn(lsmod,3),combn(lsmod,4))
lsmod.combin
