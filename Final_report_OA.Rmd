---
title: "Final_report"
author: "Ozan Aygun"
date: "4/3/2017"
output: html_document
---

# Summary


# Data loading and partitioning

Download the data sets:

```{r,results='markup',eval=FALSE}
fileURLTrain <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
fileURLTest <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(fileURLTrain,"train.csv")
download.file(fileURLTest,"test.csv")
```

Load the datasets:

```{r,results='markup',eval=FALSE}
training <- read.csv("train.csv")
final_testing <- read.csv("test.csv")
```

Understanding the classes:

Based on the original data description, we notice that the **classe** variable is the outcome variable representing: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes.

We will use the training set to build a predictive model in order to classify the cases in the final_testing set.

We therefore partition the **training set** into:

- **building**: actual model building set
- **tune.testing**: testing set for tuning the build models
- **validation**: for one-time evaluation of the model performance

```{r,results='markup', message=FALSE,warning=FALSE,cache=TRUE}
library(caret);library(ggplot2); set.seed(23445)
INbuilding <- createDataPartition(y = training$classe,p=0.6,list = FALSE)
building <- training[INbuilding,]

rest <- training[-INbuilding,]

INtune <- createDataPartition(y = rest$classe,p=0.5,list = FALSE)
tune.testing <- rest[INtune,]
validation <- rest[-INtune,]
```

# Data cleaning, dimension reduction and exploratory analysis

** All data processing and exploration initially performed in the building set, then exactly applied to tune.testing,validation, and final_testing sets with the same parameters.**

## STEP1: Handling missing values

Note that classes are slightly unbalanced, we have more of class A.

```{r,results='markup', message=FALSE,warning=FALSE,cache=TRUE}
table(building$classe) 
apply(is.na(building),2,sum)
length(which(apply(is.na(building),2,sum)>0)) 
```

Note that 11548 data points (98% of them) are consistently missing in 67 variables. 

```{r,results='markup', message=FALSE,warning=FALSE,cache=TRUE}
table(building$classe[is.na(building$amplitude_pitch_forearm)])/table(building$classe) 
```
The missing values seem to be balanced between the classes, similar fraction of classes are contained in missing values.

Therefore, drop the missing value containing columns as they unlikely to contribute our predictive power.
```{r,results='markup', message=FALSE,warning=FALSE,cache=TRUE}
tune.testing <- tune.testing[,-which(apply(is.na(building),2,sum)>0)]
validation <- validation[,-which(apply(is.na(building),2,sum)>0)]
final_testing <- final_testing[,-which(apply(is.na(building),2,sum)>0)]
building <- building[,-which(apply(is.na(building),2,sum)>0)]
```

## STEP 2: Remove near-zero variance features

Therefore, drop the missing value containing columns as they unlikely to contribute our predictive power.
```{r,results='markup', message=FALSE,warning=FALSE,cache=TRUE}
nsv <- nearZeroVar(x = building, saveMetrics = TRUE)
sum(!nsv$nzv)  
```
59 of of 93 remaining featured have non-zero variance and will be kept in the building set.


# Model training, testing and stacking

# Final model validation: prediction of 20 test cases