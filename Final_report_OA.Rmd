---
title: "Final_report"
author: "Ozan Aygun"
date: "4/3/2017"
output: html_document
---

# Summary


# Data loading and partitioning

Download the data sets:

```{r,results='markup',eval=FALSE}
fileURLTrain <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
fileURLTest <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(fileURLTrain,"train.csv")
download.file(fileURLTest,"test.csv")
```

Load the datasets:

```{r,results='markup',eval=FALSE}
training <- read.csv("train.csv")
final_testing <- read.csv("test.csv")
```

Understanding the classes:

Based on the original data description, we notice that the **classe** variable is the outcome variable representing: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes.

We will use the training set to build a predictive model in order to classify the cases in the final_testing set.

We therefore partition the **training set** into:

- **building**: actual model building set
- **tune.testing**: testing set for tuning the build models
- **validation**: for one-time evaluation of the model performance

```{r,results='markup', message=FALSE,warning=FALSE,cache=TRUE}
library(caret);library(ggplot2); set.seed(23445)
INbuilding <- createDataPartition(y = training$classe,p=0.6,list = FALSE)
building <- training[INbuilding,]

rest <- training[-INbuilding,]

INtune <- createDataPartition(y = rest$classe,p=0.5,list = FALSE)
tune.testing <- rest[INtune,]
validation <- rest[-INtune,]
```

# Data cleaning, dimension reduction and exploratory analysis

** All data processing and exploration initially performed in the building set, then exactly applied to tune.testing,validation, and final_testing sets with the same parameters.**

## STEP1: Handling missing values

Note that classes are slightly unbalanced, we have more of class A.

```{r,results='markup', message=FALSE,warning=FALSE,cache=TRUE}
table(building$classe) 
apply(is.na(building),2,sum)
length(which(apply(is.na(building),2,sum)>0)) 
```

Note that 11548 data points (98% of them) are consistently missing in 67 variables. 

```{r,results='markup', message=FALSE,warning=FALSE,cache=TRUE}
table(building$classe[is.na(building$amplitude_pitch_forearm)])/table(building$classe) 
```
The missing values seem to be balanced between the classes, similar fraction of classes are contained in missing values.

Therefore, drop the missing value containing columns as they unlikely to contribute our predictive power.
```{r,results='markup', message=FALSE,warning=FALSE,cache=TRUE}
tune.testing <- tune.testing[,-which(apply(is.na(building),2,sum)>0)]
validation <- validation[,-which(apply(is.na(building),2,sum)>0)]
final_testing <- final_testing[,-which(apply(is.na(building),2,sum)>0)]
building <- building[,-which(apply(is.na(building),2,sum)>0)]
```

## STEP 2: Remove near-zero variance features

Therefore, drop the missing value containing columns as they unlikely to contribute our predictive power.
```{r,results='markup', message=FALSE,warning=FALSE,cache=TRUE}
nsv <- nearZeroVar(x = building, saveMetrics = TRUE)
sum(!nsv$nzv)  
```
59 of the 93 remaining features have non-zero variance and will be kept the data sets.
```{r,results='markup', message=FALSE,warning=FALSE,cache=TRUE}
tune.testing <- tune.testing[,!nsv$nzv]
validation <- validation[,!nsv$nzv]
final_testing <- final_testing[,!nsv$nzv]
building <- building[,!nsv$nzv] 
```

## Exploratory data analysis

####Investigating collinear features

```{r,results='markup', message=FALSE,warning=FALSE,cache=TRUE}
cont <- !sapply(building,is.factor) # Continuous variables in the building set
M <- abs(cor(building[,cont])) # M is an absolute value correlation matrix representing the pairwise #correlations between all continuous variables 
diag(M) <- 0 # We replace the diagonal values with zero (just because these are the correations with  #themselves we are not interested in capturing them).
which(M > 0.8, arr.ind = TRUE) # What are the highest correated variables?
unique(row.names(which(M > 0.8, arr.ind = TRUE)))  
```

We find that there are 18 highly correlated predictiors in the data set. If we further explore one of them:

```{r,results='markup', message=FALSE,warning=FALSE,cache=TRUE, fig.align='center'}
qplot(building[,row.names(M)[7]],building[,colnames(M)[5]], color = classe, data = building)+theme_bw()
```

#### Principal components analysis (PCA)

We perform PCA to see if dimension reduction might help to resolve classes in the case of collinear features:

```{r,results='markup', message=FALSE,warning=FALSE,cache=TRUE, fig.align='center'}
cor.variables <- building[,unique(row.names(which(M > 0.8, arr.ind = TRUE)))]
cor.variables$classe <- building$classe

prePCA <- preProcess(cor.variables[,-19],method = "pca")
PCAcor <- predict(prePCA,cor.variables[,-19])
qplot(PCAcor$PC1,PCAcor$PC2, color = classe, data = cor.variables) +theme_bw()
```

We concluded that there is no obvious advantage gained by calculating principal components for the collinear features. Importantly, we notice that the correlated predictors have already clusters within them. In this case, we will not attempt further dimension reduction, because the data looks like suitable for classification algorithms (rather than linear models).

## STEP3: Feature engineering

Converting factor variables into dummy variables. 

```{r,results='markup', message=FALSE,warning=FALSE,cache=TRUE, fig.align='center'}
which(sapply(building[,-59],is.factor)) # Only two factor variables remained in the datasets
# Note that one of them is a time stamp composed of 20 unique values. At this point we will model them as #categorical variables
factors <- which(sapply(building[,-59],is.factor))
# Convert these variables into dummy variables:
dummies <- dummyVars(classe ~ user_name + cvtd_timestamp, data = building)

# Add them into building and all test and validation sets and drop the original factor variables
building <- cbind(building[,-factors],predict(dummies,building))
tune.testing <- cbind(tune.testing[,-factors],predict(dummies,tune.testing))
validation <- cbind(validation[,-factors],predict(dummies,validation))
names(final_testing)[59] <- "classe" # Names in newdata should match the object to use predict( ) function
final_testing <- cbind(final_testing[,-factors],predict(dummies,final_testing))
```

## STEP4: Eliminating the bias from the indexing variable X

We notice that the variable X is just an indexing variable that can *perfectly* seperate the classes:

```{r,results='markup', message=FALSE,warning=FALSE,cache=TRUE, fig.align='center'}
qplot(X,classe,data = building,color = classe)+theme_bw()
```

Such a variable has no real predictive power because the real samples will not be ordered based on such an index. Therefore, we remove the variable X from all sets before building the models:

```{r,results='markup', message=FALSE,warning=FALSE,cache=TRUE, fig.align='center'}
building <- subset(building,select = -c(X))
tune.testing <- subset(tune.testing,select = -c(X))
validation <- subset(validation,select = -c(X))
final_testing <- subset(final_testing,select = -c(X))
```

## STEP5 : legitimize the feature names

When performing feature creation, particularly for the creation of the dummy variables, we generated complex feature names. The classification algorithms in R can not handle such comples feature names that contains special characters such as **"/"**. Therefore, we legitimize all feature names before model training:

```{r,results='markup', message=FALSE,warning=FALSE,cache=TRUE, fig.align='center'}
names(building) <- make.names(names(building))
names(tune.testing) <- make.names(names(tune.testing))
names(validation) <- make.names(names(validation))
names(final_testing) <- make.names(names(final_testing))
```

# Model training, testing and stacking

Since this is a very intuitive classification problem and there are clear non-linear trends/structures/clusters in the data, we will build tree-based classifiers and support vector machines to perform predictions.

We will first build stand-alone classifiers, then stack them by using the respective algorithms. We will evaluate the predictive performance of the individual as well as the stacked classifiers by using the validation data set.

###Cross validation statement

In order to avoid overfitting of the classifiers, we will perform 10-fold cross-validation while building each individual classifier, as well as when stacking them together. Given the high number of dimensions - high number of features in the data set, this incorporates a substantial computational expense to build our classifiers. However, this was necessary to obtain the high accuracy and sucessfull classifiers as demonstrated below. 

## Building Individual classifiers:

**Simple classification tree (method = "rpart") with cross validation:**

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE,eval=FALSE}
set.seed(125745)
RPART <- train(classe ~ ., data = building,method = "rpart", trControl = trainControl(method = "cv", number = 10))
saveRDS(RPART,"RPART.rds") #Save model object for future loading if necessary
```

**Random Forest (method = "cv") with cross validation**

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE,eval=FALSE}
set.seed(125745)
RF <- train(classe ~ ., data = building,method = "rf", trControl = trainControl(method = "cv", number = 10)) # Takes very long time to run!!
saveRDS(RF,"RF.rds") #Save model object for future loading if necessary

```


# Final model validation: prediction of 20 test cases